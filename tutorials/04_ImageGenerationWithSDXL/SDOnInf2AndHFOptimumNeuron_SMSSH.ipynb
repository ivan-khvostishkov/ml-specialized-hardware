{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c36d0-f910-4288-9527-cb8b75a59c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy SD2.1 to Inferentia2 + SageMaker + HF Optimum Neuron + SageMaker SSH Helper\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "SageMaker SSH Helper is the \"army-knife\" library that helps you to securely connect to training jobs, processing jobs, batch inference jobs and realtime inference endpoints as well as SageMaker Studio Notebooks and SageMaker Notebook Instances for fast interactive experimentation, remote debugging, and advanced troubleshooting.\n",
    "\n",
    "\n",
    "Learn more about remote development and debugging with SageMaker SSH Helper in the GitHub repository: [https://github.com/aws-samples/sagemaker-ssh-helper](https://github.com/aws-samples/sagemaker-ssh-helper)\n",
    "\n",
    "In this example we use SageMaker SSH Helper to connect to remote endpoint instance running on ml.inf2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc514194-480e-422c-b9be-c39bf4b48760",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Update SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf072e-8c44-4394-8ca2-229db89915ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "%pip install sagemaker-ssh-helper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef2b3bd45ea52ca"
  },
  {
   "cell_type": "markdown",
   "id": "9d0dfa12-a2e7-479b-8c28-1fd962668e5f",
   "metadata": {},
   "source": [
    "## 2) Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869de2b-9b9b-4942-8ce6-ce1e51f2580e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-2'\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ad087-6f22-4513-9550-c23911026de7",
   "metadata": {},
   "source": [
    "## 3) Create artifacts to compile & run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6fd17-90b6-4734-913e-c52695384d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:15:45.398156Z",
     "start_time": "2023-11-29T14:15:45.369888Z"
    }
   },
   "source": [
    "### 3.1) Dependencies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1e5d-76d3-4940-aeef-09feede76d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize src_ssh/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eff2d-31a7-49fd-89ab-23a800bb0814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2) Python script for compiling and deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285bd5-3b17-47f9-96d0-b38deb5c3311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:16:04.098067Z",
     "start_time": "2023-11-29T14:16:04.074059Z"
    }
   },
   "source": [
    "This script will download model weights from HF, compile each module to inf2 and save the compiled artifacts to S3\n",
    "\n",
    "The envvar **NEURON_RT_NUM_CORES** controls how many NeuronCores are allocated per process. SageMaker can launch multiple processes in just one Endpoint. It means you can increase throughput of your endpoint by deploying your model to a larger instance (inf2.24xlarge or inf2.48xlarge). On an inf2.24xlarge, for instance, SageMaker can deploy 6 copies of the same model in parallel to serve 6 simultaneous clients. This is what we'll do in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a15a-1a24-4248-b0aa-1e3031000eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize src_ssh/compile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab929116-c7b6-473c-a21d-f0c6d908fa8b",
   "metadata": {},
   "source": [
    "## 4) SageMaker (training) Job that will download and compile the model\n",
    "\n",
    "**ATTENTION**: You need to run this step only once. Then, you can deploy the compiled model as many times as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132be8-9b37-4ec9-8735-94e7d7e6e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src_ssh\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.13.2-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 128,\n",
    "    hyperparameters={\n",
    "        \"model_id\": \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workaround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "from sagemaker_ssh_helper.wrapper import SSHEstimatorWrapper \n",
    "ssh_wrapper = SSHEstimatorWrapper.create(estimator, connection_wait_time_seconds=600) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f0e337086bb5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744806-f877-4025-b1c4-c0fdad17105c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes ~66.16mins on a trn1.32xlarge\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ssh_wrapper.print_ssh_info()\n",
    "ssh_wrapper.wait_training_job()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dae358906627fbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dabdf-2641-4ab7-839b-288477b393af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "If you decide to run this notebook again, you don't need to re-compile the model.\n",
    "Just keep the following path and use it to deploy the model next time.\n",
    "\"\"\")\n",
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8775b-6b72-4fea-a120-9285b93a1676",
   "metadata": {},
   "source": [
    "## 5) Deploy the compiled model to a SageMaker endpoint on inf2\n",
    "SageMaker can launch multiple workers, depending on the size of the Inf2 instance. A worker is a standalone Python process that manages one copy of the model. SageMaker puts a load balancer on top of all these processes and distributes the load automatically for your clients. It means that you can increase throughput by launching multiple workers, which serve different clients in parallel.\n",
    "\n",
    "For instance. If you deploy the model to a **ml.inf2.48xlarge**, SageMaker can launch 12 workers with 12 copies of the model. This instance has 24 cores and each copy of the model utilizes 2 cores. Then, you can have 12 simultaneous clients invoking the endpoint and being served at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29628ca3-fa0e-4fb3-a2ba-c239b41d2f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "instance_type_idx = 1\n",
    "instance_types = ['ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_cores = [2,12,24]\n",
    "num_workers = num_cores[instance_type_idx]//2\n",
    "\n",
    "model_data = estimator.model_data\n",
    "#model_data = {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-2-555555555555/output/pytorch-training-neuronx-2023-11-29-15-32-12-256/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
    "\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.14.1-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('sd'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "    },\n",
    "    # for production, it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "from sagemaker_ssh_helper.wrapper import SSHModelWrapper\n",
    "ssh_wrapper = SSHModelWrapper.create(pytorch_model, connection_wait_time_seconds=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36a7ec33b5ba796"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325d17-aa4e-4cd3-8190-156984cfdac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "ssh_wrapper.print_ssh_info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea3e7783d45960c1"
  },
  {
   "cell_type": "markdown",
   "id": "c191daec-2ab9-42db-b237-76b110b82ea3",
   "metadata": {},
   "source": [
    "## 6) Run a simple test to check the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd892b26-27c5-4e05-8096-f953e1ee4142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = BytesDeserializer(accept='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f196a1-51a8-4edc-a573-aa6aa6fe65a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# adjust this number according to the instance size and number of workers\n",
    "num_clients = num_workers\n",
    "input_req = {\n",
    "    \"prompt\": \"A giant crocodile crawling through the jungles with people riding on his top. The crocodile the size of a truck, with his paws serving as the truck wheels\",\n",
    "    # more info about these 2 params here: https://huggingface.co/blog/stable_diffusion\n",
    "    \"num_inference_steps\": 25,\n",
    "    \"guidance_scale\": 7.5\n",
    "}\n",
    "input_reqs = [input_req] * num_clients\n",
    "\n",
    "def predict(req):    \n",
    "    data = predictor.predict(req)\n",
    "    return data\n",
    "\n",
    "print(\"The model latency per prediction in BF16 is ~2.01s (50 iterations) ~1.26s (25 iterations).\")\n",
    "print(\"The time you'll get here also includes IO (data transfer in and out). You can reduce that by defining a VPC-endpoint in the PyTorchModel above.\")\n",
    "with ThreadPool(num_clients) as p:\n",
    "    t = time.time()\n",
    "    data = p.map(predict, input_reqs)\n",
    "    print(f\"{len(data)} images generated in {(time.time()-t):0.2f}s.\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "columns = 2\n",
    "rows = 3\n",
    "j = 0\n",
    "for i in range(1, columns*rows +1):    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(Image.open(io.BytesIO(data[j])))\n",
    "    j += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the endpoint (manually or scheduled)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "869b29b2e2d008c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "last_cell_timestamp = time.time()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd49d8bc5f9c3e4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're running the notebook with \"Run All Cells\" command, the above cell and the below cell will be executed automatically one by one and the endpoint will stay active for you to experiment with. It will incur additional hourly charges.\n",
    "\n",
    "However, if you run the below cell manually again after some time depending on the `grace_period_seconds` variable, the endpoint will be gracefully deleted and no further charges will occur.\n",
    "\n",
    "If you forget to delete the endpoint manually, but will keep the notebook kernel instance up and running, the endpoint will be deleted by the background thread depending on the `auto_delete_hours` variable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c45ccb2a185ffcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import threading\n",
    "import botocore.exceptions\n",
    "\n",
    "seconds_since_last_cell_timestamp = int(time.time() - last_cell_timestamp)\n",
    "grace_period_seconds = 30\n",
    "auto_delete_hours = 8.0\n",
    "\n",
    "def delete_endpoint():\n",
    "    try:\n",
    "        predictor.delete_endpoint(delete_endpoint_config=False)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # Most likely, already deleted\n",
    "        print(e)\n",
    "        pass\n",
    "    print(\"Endpoint has been deleted.\")\n",
    "    \n",
    "def schedule_auto_delete():\n",
    "    global auto_delete_timer\n",
    "    try:\n",
    "        print(\"Cancelling previous timer...\")\n",
    "        auto_delete_timer.cancel()\n",
    "    except NameError:\n",
    "        print(\"Timer hasn't been yet defined.\")\n",
    "        print(f\"Scheduling the automatic deletion in {auto_delete_hours} hours.\")\n",
    "    else:\n",
    "        print(\"Timer is cancelled.\")\n",
    "        print(f\"Re-scheduling the automatic deletion in {auto_delete_hours} hours.\")\n",
    "    auto_delete_timer = threading.Timer(auto_delete_hours * 60 * 60, delete_endpoint)\n",
    "    auto_delete_timer.start()    \n",
    "\n",
    "print(\"Checking notebook for automated run...\")\n",
    "print(f\"Seconds since last cell execution: {seconds_since_last_cell_timestamp}\")\n",
    "if seconds_since_last_cell_timestamp > grace_period_seconds:\n",
    "    print(\"Cell is executed manually. Deleting endpoint.\")\n",
    "    delete_endpoint()\n",
    "else:\n",
    "    print(\"Cell is executed automatically. Skipping endpoint delete. Don't forget to run the cell again to delete endpoint manually.\")\n",
    "    schedule_auto_delete()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4be2fbe1d00f73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't want to delete the endpoint right now, but want to extend the automatic deletion instead, run the below cell manually. The countdown for automatic deletion will start over."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cbaa5b92bb014d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seconds_since_last_cell_timestamp = int(time.time() - last_cell_timestamp)\n",
    "print(f\"Seconds since last cell execution: {seconds_since_last_cell_timestamp}\")\n",
    "\n",
    "if seconds_since_last_cell_timestamp > grace_period_seconds:\n",
    "    print(f\"Cell is executed manually. Re-scheduling the timer.\")\n",
    "    schedule_auto_delete()\n",
    "else:\n",
    "    print(\"Cell is executed automatically. Skipping re-scheduling. Run the cell again to re-schedule deletion.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2701b5462580fb08"
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
