{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c36d0-f910-4288-9527-cb8b75a59c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy SD2.1 to Inferentia2 + SageMaker + HF Optimum Neuron + SageMaker SSH Helper\n",
    "\n",
    "**SageMaker Studio Kernel**: Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)  \n",
    "**Instance**: ml.t3.medium\n",
    "\n",
    "SageMaker SSH Helper is the \"army-knife\" library that helps you to securely connect to training jobs, processing jobs, batch inference jobs and realtime inference endpoints as well as SageMaker Studio Notebooks and SageMaker Notebook Instances for fast interactive experimentation, remote debugging, and advanced troubleshooting.\n",
    "\n",
    "\n",
    "Learn more about remote development and debugging with SageMaker SSH Helper in the GitHub repository: [https://github.com/aws-samples/sagemaker-ssh-helper](https://github.com/aws-samples/sagemaker-ssh-helper)\n",
    "\n",
    "In this example we use SageMaker SSH Helper to connect to remote endpoint instance running on ml.inf2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc514194-480e-422c-b9be-c39bf4b48760",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Update SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf072e-8c44-4394-8ca2-229db89915ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker\n",
    "%pip install matplotlib\n",
    "%pip install sagemaker-ssh-helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dfa12-a2e7-479b-8c28-1fd962668e5f",
   "metadata": {},
   "source": [
    "## 2) Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869de2b-9b9b-4942-8ce6-ce1e51f2580e",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-2'\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ad087-6f22-4513-9550-c23911026de7",
   "metadata": {},
   "source": [
    "## 3) Create artifacts to compile & run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6fd17-90b6-4734-913e-c52695384d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:15:45.398156Z",
     "start_time": "2023-11-29T14:15:45.369888Z"
    }
   },
   "source": [
    "### 3.1) Dependencies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1e5d-76d3-4940-aeef-09feede76d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "evaluate==0.4.0\n",
    "diffusers==0.21.4\n",
    "accelerate==0.23.0\n",
    "scikit-learn==1.3.0\n",
    "transformers==4.33.1\n",
    "optimum-neuron==0.0.12\n",
    "sagemaker-ssh-helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eff2d-31a7-49fd-89ab-23a800bb0814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2) Python script for compiling and deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285bd5-3b17-47f9-96d0-b38deb5c3311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:16:04.098067Z",
     "start_time": "2023-11-29T14:16:04.074059Z"
    }
   },
   "source": [
    "This script will download model weights from HF, compile each module to inf2 and save the compiled artifacts to S3\n",
    "\n",
    "The envvar **NEURON_RT_NUM_CORES** controls how many NeuronCores are allocated per process. SageMaker can launch multiple processes in just one Endpoint. It means you can increase throughput of your endpoint by deploying your model to a larger instance (inf2.24xlarge or inf2.48xlarge). On an inf2.24xlarge, for instance, SageMaker can deploy 6 copies of the same model in parallel to serve 6 simultaneous clients. This is what we'll do in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065a15a-1a24-4248-b0aa-1e3031000eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "# See https://github.com/aws-samples/sagemaker-ssh-helper#inference\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), \"lib\"))\n",
    "import sagemaker_ssh_helper\n",
    "sagemaker_ssh_helper.setup_and_start_ssh()\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES'] = '2'\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import argparse\n",
    "from filelock import FileLock\n",
    "from optimum.neuron import NeuronStableDiffusionPipeline\n",
    "\n",
    "lock_path = '/tmp/new_packages.lock'\n",
    "lock = FileLock(lock_path)\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    print(\"Waiting for the lock acquire...\")    \n",
    "    lock.acquire()\n",
    "    print(\"Loading model...\")\n",
    "    t = time.time()    \n",
    "    model = NeuronStableDiffusionPipeline.from_pretrained(model_dir,  device_ids=[0, 1])\n",
    "    print(f\"Model loaded. Elapsed: {time.time()-t}s\")\n",
    "    lock.release()\n",
    "    return model\n",
    "\n",
    "    \n",
    "def input_fn(request_body, request_content_type, context=None):\n",
    "    if request_content_type == 'application/json':\n",
    "        req = json.loads(request_body)\n",
    "        prompt = req.get('prompt')\n",
    "        num_inference_steps = req.get('num_inference_steps', 50)\n",
    "        guidance_scale = req.get('guidance_scale', 7.5)\n",
    "        if prompt is None or type(prompt) != str or len(prompt) < 5:\n",
    "            raise(\"Invalid prompt. It needs to be a string > 5\")\n",
    "        if type(num_inference_steps) != int:\n",
    "            raise(\"Invalid num_inference_steps. Expected int. default = 50\")\n",
    "        if type(guidance_scale) != float:\n",
    "            raise(\"Invalid guidance_scale. Expected float. default = 7.5\")\n",
    "        return prompt, num_inference_steps, guidance_scale\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {request_content_type}. Supported: application/json\")\n",
    "\n",
    "\n",
    "def predict_fn(input_req, model, context=None):\n",
    "    prompt, num_inference_steps, guidance_scale = input_req\n",
    "    return model(\n",
    "        prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale\n",
    "    ).images[0]\n",
    "\n",
    "\n",
    "def output_fn(image, accept, context=None):\n",
    "    if accept != 'image/jpeg':\n",
    "        raise Exception(f'Invalid data type. Expected image/jpeg, got {accept}')\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, 'jpeg', icc_profile=image.info.get('icc_profile'))\n",
    "    buffer.seek(0)\n",
    "    return buffer.read()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.    \n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"stabilityai/stable-diffusion-2-1-base\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--height\", type=int, default=512)\n",
    "    parser.add_argument(\"--width\", type=int, default=512)\n",
    "    parser.add_argument(\"--auto_cast\", type=str, default=\"matmul\")\n",
    "    parser.add_argument(\"--auto_cast_type\", type=str, default=\"bf16\")\n",
    "\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    model_id = args.model_id\n",
    "    compiler_args = {\"auto_cast\": args.auto_cast, \"auto_cast_type\": args.auto_cast_type}\n",
    "    input_shapes = {\"batch_size\": args.batch_size, \"height\": args.height, \"width\": args.width}\n",
    "\n",
    "    print(\"Compiling model...\")    \n",
    "    t = time.time()\n",
    "    stable_diffusion = NeuronStableDiffusionPipeline.from_pretrained(\n",
    "        model_id, export=True, **compiler_args, **input_shapes\n",
    "    )\n",
    "    print(f\"Done. Elapsed time: {(time.time()-t) * 1000}s\")\n",
    "    print(\"Saving model...\")\n",
    "    t = time.time()\n",
    "    stable_diffusion.save_pretrained(args.model_dir)\n",
    "    print(f\"Done. Elapsed time: {(time.time()-t) * 1000}s\")\n",
    "\n",
    "    code_path = os.path.join(args.model_dir, \"code\")\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "    shutil.copy(\"compile.py\", os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy(\"requirements.txt\", os.path.join(code_path, \"requirements.txt\"))\n",
    "    print(f\"Job done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab929116-c7b6-473c-a21d-f0c6d908fa8b",
   "metadata": {},
   "source": [
    "## 4) SageMaker (training) Job that will download and compile the model\n",
    "\n",
    "**ATTENTION**: You need to run this step only once. Then, you can deploy the compiled model as many times as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49132be8-9b37-4ec9-8735-94e7d7e6e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,    \n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.13.2-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 128,\n",
    "    hyperparameters={\n",
    "        \"model_id\": \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workaround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744806-f877-4025-b1c4-c0fdad17105c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes ~66.16mins on a trn1.32xlarge\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dabdf-2641-4ab7-839b-288477b393af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "If you decide to run this notebook again, you don't need to re-compile the model.\n",
    "Just keep the following path and use it to deploy the model next time.\n",
    "\"\"\")\n",
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8775b-6b72-4fea-a120-9285b93a1676",
   "metadata": {},
   "source": [
    "## 5) Deploy the compiled model to a SageMaker endpoint on inf2\n",
    "SageMaker can launch multiple workers, depending on the size of the Inf2 instance. A worker is a standalone Python process that manages one copy of the model. SageMaker puts a load balancer on top of all these processes and distributes the load automatically for your clients. It means that you can increase throughput by launching multiple workers, which serve different clients in parallel.\n",
    "\n",
    "For instance. If you deploy the model to a **ml.inf2.48xlarge**, SageMaker can launch 12 workers with 12 copies of the model. This instance has 24 cores and each copy of the model utilizes 2 cores. Then, you can have 12 simultaneous clients invoking the endpoint and being served at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29628ca3-fa0e-4fb3-a2ba-c239b41d2f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "instance_type_idx = 1\n",
    "instance_types = ['ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_cores = [2,12,24]\n",
    "num_workers = num_cores[instance_type_idx]//2\n",
    "\n",
    "model_data = estimator.model_data\n",
    "#model_data = {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-2-555555555555/output/pytorch-training-neuronx-2023-11-29-15-32-12-256/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
    "\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.14.1-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('sd'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "    },\n",
    "    # for production, it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "from sagemaker_ssh_helper.wrapper import SSHModelWrapper\n",
    "ssh_wrapper = SSHModelWrapper.create(pytorch_model, connection_wait_time_seconds=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36a7ec33b5ba796"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2325d17-aa4e-4cd3-8190-156984cfdac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://github.com/aws-samples/sagemaker-ssh-helper\n",
    "print(f\"Remote endpoint logs are at {ssh_wrapper.get_cloudwatch_url()}\")\n",
    "print(f\"Endpoint metadata is at {ssh_wrapper.get_metadata_url()}\")\n",
    "print(f\"Endpoint config metadata is at {ssh_wrapper.get_config_metadata_url()}\")\n",
    "print(f\"Model metadata is at {ssh_wrapper.get_model_metadata_url()}\")\n",
    "print(f\"To connect over SSM run: AWS_DEFAULT_REGION={region} aws ssm start-session --target {ssh_wrapper.get_instance_ids()[0]}\")\n",
    "print(f\"To configure local host for SSH run: sm-local-configure\")\n",
    "print(f\"To connect over SSH run: AWS_DEFAULT_REGION={region} sm-ssh connect-ports {pytorch_model.endpoint_name}.inference.sagemaker\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "848ad30b9b09a4ff"
  },
  {
   "cell_type": "markdown",
   "id": "c191daec-2ab9-42db-b237-76b110b82ea3",
   "metadata": {},
   "source": [
    "## 6) Run a simple test to check the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd892b26-27c5-4e05-8096-f953e1ee4142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = BytesDeserializer(accept='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f196a1-51a8-4edc-a573-aa6aa6fe65a9",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# adjust this number according to the instance size and number of workers\n",
    "num_clients = num_workers\n",
    "input_req = {\n",
    "    \"prompt\": \"A giant crocodile crawling through the jungles with people riding on his top. The crocodile the size of a truck, with his paws serving as the truck wheels\",\n",
    "    # more info about these 2 params here: https://huggingface.co/blog/stable_diffusion\n",
    "    \"num_inference_steps\": 25,\n",
    "    \"guidance_scale\": 7.5\n",
    "}\n",
    "input_reqs = [input_req] * num_clients\n",
    "\n",
    "def predict(req):    \n",
    "    data = predictor.predict(req)\n",
    "    return data\n",
    "\n",
    "print(\"The model latency per prediction in BF16 is ~2.01s (50 iterations) ~1.26s (25 iterations).\")\n",
    "print(\"The time you'll get here also includes IO (data transfer in and out). You can reduce that by defining a VPC-endpoint in the PyTorchModel above.\")\n",
    "with ThreadPool(num_clients) as p:\n",
    "    t = time.time()\n",
    "    data = p.map(predict, input_reqs)\n",
    "    print(f\"{len(data)} images generated in {(time.time()-t):0.2f}s.\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "columns = 2\n",
    "rows = 3\n",
    "j = 0\n",
    "for i in range(1, columns*rows +1):    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(Image.open(io.BytesIO(data[j])))\n",
    "    j += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the endpoint (manually or scheduled)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "869b29b2e2d008c7"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "last_cell_timestamp = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:11:32.107293Z",
     "start_time": "2023-11-29T18:11:32.103252Z"
    }
   },
   "id": "dd49d8bc5f9c3e4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're running the notebook with \"Run All Cells\" command, the above cell and the below cell will be executed automatically one by one and the endpoint will stay active for you to experiment with. It will incur additional hourly charges.\n",
    "\n",
    "However, if you run the below cell manually again after some time depending on the `grace_period_seconds` variable, the endpoint will be gracefully deleted and no further charges will occur.\n",
    "\n",
    "If you forget to delete the endpoint manually, but will keep the notebook kernel instance up and running, the endpoint will be deleted by the background thread depending on the `auto_delete_hours` variable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c45ccb2a185ffcc"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking notebook for automated run...\n",
      "Seconds since last cell exectution: 2\n",
      "Cell is executed automatically. Skipping endpoint delete. Don't forget to run the cell again to delete endpoint manually.\n",
      "Cancelling previous timer...\n",
      "Timer is cancelled.\n",
      "Re-scheduling the automatic deletion in 8.0 hours.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import botocore.exceptions\n",
    "\n",
    "seconds_since_last_cell_timestamp = int(time.time() - last_cell_timestamp)\n",
    "grace_period_seconds = 30\n",
    "auto_delete_hours = 8.0\n",
    "\n",
    "def delete_endpoint():\n",
    "    try:\n",
    "        predictor.delete_endpoint(delete_endpoint_config=False)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # Most likely, already deleted\n",
    "        print(e)\n",
    "        pass\n",
    "    print(\"Endpoint has been deleted.\")\n",
    "    \n",
    "def schedule_auto_delete():\n",
    "    global auto_delete_timer\n",
    "    try:\n",
    "        print(\"Cancelling previous timer...\")\n",
    "        auto_delete_timer.cancel()\n",
    "    except NameError:\n",
    "        print(\"Timer hasn't been yet defined.\")\n",
    "        print(f\"Scheduling the automatic deletion in {auto_delete_hours} hours.\")\n",
    "    else:\n",
    "        print(\"Timer is cancelled.\")\n",
    "        print(f\"Re-scheduling the automatic deletion in {auto_delete_hours} hours.\")\n",
    "    auto_delete_timer = threading.Timer(auto_delete_hours * 60 * 60, delete_endpoint)\n",
    "    auto_delete_timer.start()    \n",
    "\n",
    "print(\"Checking notebook for automated run...\")\n",
    "print(f\"Seconds since last cell exectution: {seconds_since_last_cell_timestamp}\")\n",
    "if seconds_since_last_cell_timestamp > grace_period_seconds:\n",
    "    print(\"Cell is executed manually. Deleting endpoint.\")\n",
    "    delete_endpoint()\n",
    "else:\n",
    "    print(\"Cell is executed automatically. Skipping endpoint delete. Don't forget to run the cell again to delete endpoint manually.\")\n",
    "    schedule_auto_delete()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:11:34.227727Z",
     "start_time": "2023-11-29T18:11:34.208169Z"
    }
   },
   "id": "6a4be2fbe1d00f73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't want to delete the endpoint right now, but want to extend the automatic deletion instead, run the below cell manually. The countdown for automatic deletion will start over."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cbaa5b92bb014d5"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds since last cell exectution: 7\n",
      "Cell is executed automatically. Skipping re-scheduling. Run the cell again to re-schedule deletion.\n"
     ]
    }
   ],
   "source": [
    "seconds_since_last_cell_timestamp = int(time.time() - last_cell_timestamp)\n",
    "print(f\"Seconds since last cell exectution: {seconds_since_last_cell_timestamp}\")\n",
    "\n",
    "if seconds_since_last_cell_timestamp > grace_period_seconds:\n",
    "    print(f\"Cell is executed manually. Re-scheduling the timer.\")\n",
    "    schedule_auto_delete()\n",
    "else:\n",
    "    print(\"Cell is executed automatically. Skipping re-scheduling. Run the cell again to re-schedule deletion.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:11:39.467183Z",
     "start_time": "2023-11-29T18:11:39.423438Z"
    }
   },
   "id": "2701b5462580fb08"
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
